{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AFRIGAN _Train  .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU6QUP-XKQx0",
        "outputId": "9b6de4b9-9220-4723-98fa-c6f99a22ac4b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxU3pFWtp01y"
      },
      "source": [
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VntTOppW7PX"
      },
      "source": [
        "#if your images format is in png , convert to jpg (optional)\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "directory = r\"\" #directory that contains your png format images \n",
        "new_path=r\"\" #target directory \n",
        "c=1\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".png\"):\n",
        "        print(filename)\n",
        "        im = Image.open(r\"\"+filename)#former directory \n",
        "        name=''+str(c)+'.jpg'\n",
        "        rgb_im = im.convert('RGB')\n",
        "        rgb_im.save(new_path+\"/\"+name)\n",
        "        c+=1\n",
        "        print(os.path.join(new_path, filename))\n",
        "        continue\n",
        "    else:\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EObA3P7NoIS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e06aec45-e853-44a3-c27e-c5da5a954e76"
      },
      "source": [
        "!nvidia-smi\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep 16 10:34:15 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxxYlEKI9Gis"
      },
      "source": [
        "#Loading from stylegan\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epV6TDzAjox1"
      },
      "source": [
        "Next, run this cell. If you’re already installed the repo, it will skip the installation process and change into the repo’s directory. If you haven’t installed it, it will install all the files necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxEC0n9u9mAH",
        "outputId": "9b25ef07-2664-4353-e8fe-a02a9d11e583"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/AFRIGAN/stylegan2-ada\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1uxYpv9sJe8NkGfWLb5-a0UfDwdQpHuPz/AFRIGAN/colab-sg2-ada/stylegan2-ada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeS9tDvt61VG"
      },
      "source": [
        "## Convert dataset to .tfrecords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q58MJbckLUc"
      },
      "source": [
        "\n",
        "\n",
        "we need to convert our image dataset to a format that StyleGAN2-ADA can read \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w22IZthdX_P",
        "outputId": "5e2302d8-7ce6-42c4-ec66-805384e5e95f"
      },
      "source": [
        "%tensorflow_version 1.x\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44VMzZ7jCf1c"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm1SD4SBMi5P",
        "outputId": "d2511ac4-6a78-4db4-9ed1-4049208662eb"
      },
      "source": [
        "#update this to the path to your image folder\n",
        "dataset_path = \"/content/drive/MyDrive/AFRIGAN/project2/AFRIFASHION1600\"     #input the link to your dataset \n",
        "#give your dataset a name\n",
        "dataset_name = \"AFRICANFASHION\"\n",
        "\n",
        "#output file \n",
        "tfrecords_path = \" /content/drive/MyDrive/AFRIGAN/Training_AFRIGAN\"\n",
        "\n",
        "#covert to tfrecords \n",
        "!python dataset_tool.py  create_from_images {tfrecords_path}/{dataset_name} {dataset_path}"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from \"/content/drive/MyDrive/AFRIGAN/project2/AFRIFASHION1600\"\n",
            "Creating dataset \"/content/drive/MyDrive/AFRIGAN/Training_AFRIGAN/AFRICANFASHION\"\n",
            "0 / 1595\rdataset_tool.py:97: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[quant.tostring()]))}))\n",
            "Added 1595 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DvTupHzP2s_"
      },
      "source": [
        "## Training AFRIGAN model \n",
        "\n",
        "\n",
        "we will be using ffhq model on [styleganADA ](https://github.com/NVlabs/stylegan2-ada\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxu7CA0Qb1Yd"
      },
      "source": [
        "!python train.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOftFoyiDU3s",
        "outputId": "975cbdeb-6832-42f1-cc0a-5d2de26e7526"
      },
      "source": [
        "\n",
        "snapshot_count = 8 #model generate samples and network file every 8 ticks interval \n",
        "#should the images be mirrored left to right?\n",
        "mirrored = True\n",
        "\n",
        "mirroredY = False #images will be mirrored top to bottom \n",
        " \n",
        "metric_list = None #set metrics \n",
        "\n",
        "resume_from =\"ffhq512\"\n",
        "\n",
        "#don't edit this unless you know what you're doing :)\n",
        "! python train.py --outdir ./new_con --snap={snapshot_count} --cfg=11gb-gpu --kimg=5000  --data=/content/drive/MyDrive/AFRIGAN/colab-sg2-ada/stylegan2-ada/data/AFRICANFASHION  --aug=\"ada\" --augpipe=\"bgcfnc\" --mirror={mirrored} --mirrory={mirroredY} --metrics={metric_list} --resume={resume_from}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcmalloc: large alloc 4294967296 bytes == 0x55d043824000 @  0x7f13570d1001 0x7f13542d454f 0x7f1354324b58 0x7f1354328b17 0x7f13543c7203 0x55d03b9490a4 0x55d03b948da0 0x55d03b9bd868 0x55d03b9b8235 0x55d03b94afec 0x55d03b98bbc9 0x55d03b988ac4 0x55d03b9498a9 0x55d03b9bdb0a 0x55d03b9b7c35 0x55d03b889e2c 0x55d03b9ba318 0x55d03b9b7c35 0x55d03b889e2c 0x55d03b9ba318 0x55d03b9b8235 0x55d03b889e2c 0x55d03b9ba318 0x55d03b94a65a 0x55d03b9b8b0e 0x55d03b9b7c35 0x55d03b9b7933 0x55d03ba81402 0x55d03ba8177d 0x55d03ba81626 0x55d03ba59313\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55d143824000 @  0x7f13570cf1e7 0x7f13542d446e 0x7f1354324c7b 0x7f135432535f 0x7f13543c7103 0x55d03b9490a4 0x55d03b948da0 0x55d03b9bd868 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9b993b 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9b993b 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9b993b 0x55d03b94a65a 0x55d03b9b8b0e 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9bcf40 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9b993b 0x55d03b9b8235 0x55d03b94afec 0x55d03b98bbc9 0x55d03b988ac4 0x55d03b9498a9 0x55d03b9bdb0a\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55d2448c0000 @  0x7f13570cf1e7 0x7f13542d446e 0x7f1354324c7b 0x7f135432535f 0x7f12ffcd4235 0x7f12ff657792 0x7f12ff657d42 0x7f12ff610aee 0x55d03b948f97 0x55d03b948da0 0x55d03b9bd2f9 0x55d03b94a65a 0x55d03b9b8d67 0x55d03b9b7dcc 0x55d03b889eb1 0x55d03b9ba318 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9b8d67 0x55d03b9b8235 0x55d03b94a73a 0x55d03b9b8d67 0x55d03b94a65a 0x55d03b9b8d67 0x55d03b9b7c35 0x55d03b94add1 0x55d03b94b1f1 0x55d03b9ba318 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9b8b0e\n",
            "\n",
            "Training options:\n",
            "{\n",
            "  \"G_args\": {\n",
            "    \"func_name\": \"training.networks.G_main\",\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"mapping_layers\": 8,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"D_args\": {\n",
            "    \"func_name\": \"training.networks.D_main\",\n",
            "    \"mbstd_group_size\": 4,\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.002\n",
            "  },\n",
            "  \"D_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.002\n",
            "  },\n",
            "  \"loss_args\": {\n",
            "    \"func_name\": \"training.loss.stylegan2\",\n",
            "    \"r1_gamma\": 10\n",
            "  },\n",
            "  \"augment_args\": {\n",
            "    \"class_name\": \"training.augment.AdaptiveAugment\",\n",
            "    \"tune_heuristic\": \"rt\",\n",
            "    \"tune_target\": 0.6,\n",
            "    \"apply_func\": \"training.augment.augment_pipeline\",\n",
            "    \"apply_args\": {\n",
            "      \"xflip\": 1,\n",
            "      \"rotate90\": 1,\n",
            "      \"xint\": 1,\n",
            "      \"scale\": 1,\n",
            "      \"rotate\": 1,\n",
            "      \"aniso\": 1,\n",
            "      \"xfrac\": 1,\n",
            "      \"brightness\": 1,\n",
            "      \"contrast\": 1,\n",
            "      \"lumaflip\": 1,\n",
            "      \"hue\": 1,\n",
            "      \"saturation\": 1,\n",
            "      \"imgfilter\": 1,\n",
            "      \"noise\": 1,\n",
            "      \"cutout\": 1\n",
            "    },\n",
            "    \"tune_kimg\": 100\n",
            "  },\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 4,\n",
            "  \"network_snapshot_ticks\": 4,\n",
            "  \"train_dataset_args\": {\n",
            "    \"path\": \"/content/drive/MyDrive/AFRIGAN/colab-sg2-ada/stylegan2-ada/data/AFRICANFASHION\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 512,\n",
            "    \"mirror_augment\": true,\n",
            "    \"mirror_augment_v\": false\n",
            "  },\n",
            "  \"metric_arg_list\": [],\n",
            "  \"metric_dataset_args\": {\n",
            "    \"path\": \"/content/drive/MyDrive/AFRIGAN/colab-sg2-ada/stylegan2-ada/data/AFRICANFASHION\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 512,\n",
            "    \"mirror_augment\": true,\n",
            "    \"mirror_augment_v\": false\n",
            "  },\n",
            "  \"total_kimg\": 5000,\n",
            "  \"minibatch_size\": 4,\n",
            "  \"minibatch_gpu\": 4,\n",
            "  \"G_smoothing_kimg\": 10,\n",
            "  \"G_smoothing_rampup\": null,\n",
            "  \"resume_pkl\": \"/content/drive/MyDrive/AFRIGAN/colab-sg2-ada/stylegan2-ada/new_con/00001-AFRICANFASHION-mirror-11gb-gpu-kimg5000-ada-bgcfnc-resumecustom/network-snapshot-000048.pkl\",\n",
            "  \"run_dir\": \"./new_con/00002-AFRICANFASHION-mirror-11gb-gpu-kimg5000-ada-bgcfnc-resumecustom\"\n",
            "}\n",
            "\n",
            "Output directory:  ./new_con/00002-AFRICANFASHION-mirror-11gb-gpu-kimg5000-ada-bgcfnc-resumecustom\n",
            "Training data:     /content/drive/MyDrive/AFRIGAN/colab-sg2-ada/stylegan2-ada/data/AFRICANFASHION\n",
            "Training length:   5000 kimg\n",
            "Resolution:        512\n",
            "Number of GPUs:    1\n",
            "\n",
            "Creating output directory...\n",
            "Loading training set...\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55d043762000 @  0x7f13570d1001 0x7f13542d454f 0x7f1354324b58 0x7f1354328b17 0x7f13543c7203 0x55d03b9490a4 0x55d03b948da0 0x55d03b9bd868 0x55d03b9b8235 0x55d03b94afec 0x55d03b98bbc9 0x55d03b988ac4 0x55d03b9498a9 0x55d03b9bdb0a 0x55d03b9b7c35 0x55d03b889e2c 0x55d03b9ba318 0x55d03b9b7c35 0x55d03b889e2c 0x55d03b9ba318 0x55d03b9b8235 0x55d03b889e2c 0x55d03b9ba318 0x55d03b94a65a 0x55d03b9b8b0e 0x55d03b9b7c35 0x55d03b9b7933 0x55d03ba81402 0x55d03ba8177d 0x55d03ba81626 0x55d03ba59313\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55d3448c0000 @  0x7f13570cf1e7 0x7f13542d446e 0x7f1354324c7b 0x7f135432535f 0x7f13543c7103 0x55d03b9490a4 0x55d03b948da0 0x55d03b9bd868 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9b993b 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9b993b 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9b993b 0x55d03b94a65a 0x55d03b9b8b0e 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9bcf40 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9b993b 0x55d03b9b8235 0x55d03b94afec 0x55d03b98bbc9 0x55d03b988ac4 0x55d03b9498a9 0x55d03b9bdb0a\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55d3448c0000 @  0x7f13570cf1e7 0x7f13542d446e 0x7f1354324c7b 0x7f135432535f 0x7f12ffcd4235 0x7f12ff657792 0x7f12ff657d42 0x7f12ff610aee 0x55d03b948f97 0x55d03b948da0 0x55d03b9bd2f9 0x55d03b94a65a 0x55d03b9b8d67 0x55d03b9b7dcc 0x55d03b889eb1 0x55d03b9ba318 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9b8d67 0x55d03b9b8235 0x55d03b94a73a 0x55d03b9b8d67 0x55d03b94a65a 0x55d03b9b8d67 0x55d03b9b7c35 0x55d03b94add1 0x55d03b94b1f1 0x55d03b9ba318 0x55d03b9b7c35 0x55d03b94a73a 0x55d03b9b8b0e\n",
            "Image shape: [3, 512, 512]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Compiling... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Compiling... Loading... Done.\n",
            "Resuming from \"/content/drive/MyDrive/AFRIGAN/colab-sg2-ada/stylegan2-ada/new_con/00001-AFRICANFASHION-mirror-11gb-gpu-kimg5000-ada-bgcfnc-resumecustom/network-snapshot-000048.pkl\"\n",
            "\n",
            "G                             Params    OutputShape         WeightShape     \n",
            "---                           ---       ---                 ---             \n",
            "latents_in                    -         (?, 512)            -               \n",
            "labels_in                     -         (?, 0)              -               \n",
            "epochs                        1         ()                  ()              \n",
            "epochs_1                      1         ()                  ()              \n",
            "G_mapping/Normalize           -         (?, 512)            -               \n",
            "G_mapping/Dense0              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense1              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense2              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense3              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense4              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense5              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense6              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Dense7              262656    (?, 512)            (512, 512)      \n",
            "G_mapping/Broadcast           -         (?, 16, 512)        -               \n",
            "dlatent_avg                   -         (512,)              -               \n",
            "Truncation/Lerp               -         (?, 16, 512)        -               \n",
            "G_synthesis/4x4/Const         8192      (?, 512, 4, 4)      (1, 512, 4, 4)  \n",
            "G_synthesis/4x4/Conv          2622465   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
            "G_synthesis/4x4/ToRGB         264195    (?, 3, 4, 4)        (1, 1, 512, 3)  \n",
            "G_synthesis/8x8/Conv0_up      2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Conv1         2622465   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Upsample      -         (?, 3, 8, 8)        -               \n",
            "G_synthesis/8x8/ToRGB         264195    (?, 3, 8, 8)        (1, 1, 512, 3)  \n",
            "G_synthesis/16x16/Conv0_up    2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Conv1       2622465   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Upsample    -         (?, 3, 16, 16)      -               \n",
            "G_synthesis/16x16/ToRGB       264195    (?, 3, 16, 16)      (1, 1, 512, 3)  \n",
            "G_synthesis/32x32/Conv0_up    2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Conv1       2622465   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Upsample    -         (?, 3, 32, 32)      -               \n",
            "G_synthesis/32x32/ToRGB       264195    (?, 3, 32, 32)      (1, 1, 512, 3)  \n",
            "G_synthesis/64x64/Conv0_up    2622465   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Conv1       2622465   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Upsample    -         (?, 3, 64, 64)      -               \n",
            "G_synthesis/64x64/ToRGB       264195    (?, 3, 64, 64)      (1, 1, 512, 3)  \n",
            "G_synthesis/128x128/Conv0_up  1442561   (?, 256, 128, 128)  (3, 3, 512, 256)\n",
            "G_synthesis/128x128/Conv1     721409    (?, 256, 128, 128)  (3, 3, 256, 256)\n",
            "G_synthesis/128x128/Upsample  -         (?, 3, 128, 128)    -               \n",
            "G_synthesis/128x128/ToRGB     132099    (?, 3, 128, 128)    (1, 1, 256, 3)  \n",
            "G_synthesis/256x256/Conv0_up  426369    (?, 128, 256, 256)  (3, 3, 256, 128)\n",
            "G_synthesis/256x256/Conv1     213249    (?, 128, 256, 256)  (3, 3, 128, 128)\n",
            "G_synthesis/256x256/Upsample  -         (?, 3, 256, 256)    -               \n",
            "G_synthesis/256x256/ToRGB     66051     (?, 3, 256, 256)    (1, 1, 128, 3)  \n",
            "G_synthesis/512x512/Conv0_up  139457    (?, 64, 512, 512)   (3, 3, 128, 64) \n",
            "G_synthesis/512x512/Conv1     69761     (?, 64, 512, 512)   (3, 3, 64, 64)  \n",
            "G_synthesis/512x512/Upsample  -         (?, 3, 512, 512)    -               \n",
            "G_synthesis/512x512/ToRGB     33027     (?, 3, 512, 512)    (1, 1, 64, 3)   \n",
            "---                           ---       ---                 ---             \n",
            "Total                         30276585                                      \n",
            "\n",
            "\n",
            "D                    Params    OutputShape         WeightShape     \n",
            "---                  ---       ---                 ---             \n",
            "images_in            -         (?, 3, 512, 512)    -               \n",
            "labels_in            -         (?, 0)              -               \n",
            "512x512/FromRGB      256       (?, 64, 512, 512)   (1, 1, 3, 64)   \n",
            "512x512/Conv0        36928     (?, 64, 512, 512)   (3, 3, 64, 64)  \n",
            "512x512/Conv1_down   73856     (?, 128, 256, 256)  (3, 3, 64, 128) \n",
            "512x512/Skip         8192      (?, 128, 256, 256)  (1, 1, 64, 128) \n",
            "256x256/Conv0        147584    (?, 128, 256, 256)  (3, 3, 128, 128)\n",
            "256x256/Conv1_down   295168    (?, 256, 128, 128)  (3, 3, 128, 256)\n",
            "256x256/Skip         32768     (?, 256, 128, 128)  (1, 1, 128, 256)\n",
            "128x128/Conv0        590080    (?, 256, 128, 128)  (3, 3, 256, 256)\n",
            "128x128/Conv1_down   1180160   (?, 512, 64, 64)    (3, 3, 256, 512)\n",
            "128x128/Skip         131072    (?, 512, 64, 64)    (1, 1, 256, 512)\n",
            "64x64/Conv0          2359808   (?, 512, 64, 64)    (3, 3, 512, 512)\n",
            "64x64/Conv1_down     2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "64x64/Skip           262144    (?, 512, 32, 32)    (1, 1, 512, 512)\n",
            "32x32/Conv0          2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
            "32x32/Conv1_down     2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "32x32/Skip           262144    (?, 512, 16, 16)    (1, 1, 512, 512)\n",
            "16x16/Conv0          2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
            "16x16/Conv1_down     2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "16x16/Skip           262144    (?, 512, 8, 8)      (1, 1, 512, 512)\n",
            "8x8/Conv0            2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
            "8x8/Conv1_down       2359808   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
            "8x8/Skip             262144    (?, 512, 4, 4)      (1, 1, 512, 512)\n",
            "4x4/MinibatchStddev  -         (?, 513, 4, 4)      -               \n",
            "4x4/Conv             2364416   (?, 512, 4, 4)      (3, 3, 513, 512)\n",
            "4x4/Dense0           4194816   (?, 512)            (8192, 512)     \n",
            "Output               513       (?, 1)              (512, 1)        \n",
            "---                  ---       ---                 ---             \n",
            "Total                28982849                                      \n",
            "\n",
            "Exporting sample images...\n",
            "Replicating networks across 1 GPUs...\n",
            "Initializing augmentations...\n",
            "Setting up optimizers...\n",
            "Constructing training graph...\n",
            "Finalizing training ops...\n",
            "Initializing metrics...\n",
            "Training for 5000 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 2m 04s       sec/tick 25.2    sec/kimg 1574.39 maintenance 99.1   gpumem 7.1   augment 0.000\n",
            "tick 1     kimg 4.0      time 22m 21s      sec/tick 1200.6  sec/kimg 300.15  maintenance 16.4   gpumem 7.1   augment 0.028\n",
            "tick 2     kimg 8.0      time 42m 32s      sec/tick 1210.5  sec/kimg 302.62  maintenance 0.0    gpumem 7.1   augment 0.055\n",
            "tick 3     kimg 12.0     time 1h 02m 44s   sec/tick 1212.3  sec/kimg 303.06  maintenance 0.0    gpumem 7.1   augment 0.077\n",
            "tick 4     kimg 16.0     time 1h 22m 56s   sec/tick 1212.0  sec/kimg 302.99  maintenance 0.0    gpumem 7.1   augment 0.098\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 645, in <module>\n",
            "    main()\n",
            "  File \"train.py\", line 637, in main\n",
            "    run_training(**vars(args))\n",
            "  File \"train.py\", line 522, in run_training\n",
            "    training_loop.training_loop(**training_options)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1uxYpv9sJe8NkGfWLb5-a0UfDwdQpHuPz/AFRIGAN/colab-sg2-ada/stylegan2-ada/training/training_loop.py\", line 252, in training_loop\n",
            "    tflib.run([G_train_op, data_fetch_op])\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1uxYpv9sJe8NkGfWLb5-a0UfDwdQpHuPz/AFRIGAN/colab-sg2-ada/stylegan2-ada/dnnlib/tflib/tfutil.py\", line 33, in run\n",
            "    return tf.get_default_session().run(*args, **kwargs)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiznSeSwV40e"
      },
      "source": [
        "TRAIN ON AFRIGAN MODEL \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AEB7O8qoK9w"
      },
      "source": [
        "#download AFRIGAN moodel \n",
        "!gdown https://drive.google.com/uc?id=1-H92c9YC6AOWEtAuXnxnF5n3XeQnXf9L -O \"\"#path to save te model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2VS5lTFTOAe"
      },
      "source": [
        "this name must EXACTLY match the dataset name you used when creating the .tfrecords file\n",
        "#dataset_name = \"AFRiCANFASHION\"\n",
        "\n",
        "snapshot_count = 4  #how often should the model generate samples and a .pkl file\n",
        "#should the images be mirrored left to right?\n",
        "mirrored = True\n",
        "#should the images be mirrored top to bottom?\n",
        "mirroredY = False\n",
        "#metrics? \n",
        "metric_list = None\n",
        "\n",
        "\n",
        "# copy path to afrigan \n",
        "resume_from =\"\" \n",
        "dataset_path= \"\" #path to your coverted datset \n",
        "\n",
        "\n",
        "! python train.py --outdir ./new_con --snap={snapshot_count} --cfg=11gb-gpu --kimg=5000  --data={dataset_path}  --aug=\"ada\" --augpipe=\"bgcfnc\" --mirror={mirrored} --mirrory={mirroredY} --metrics={metric_list} --resume={resume_from}\n",
        "#resume your training from your last .pkl file saved "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZR4ireZE6JF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}