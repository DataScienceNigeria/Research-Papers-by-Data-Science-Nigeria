{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfnwh9OGJ3er/qJz9eESph"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XaEUnt0woHs7"},"outputs":[],"source":["import spacy\n","nlp = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","source":["import string\n","punct = string.punctuation"],"metadata":{"id":"vvj9P93coWXl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from spacy.lang.en.stop_words import STOP_WORDS\n","stopwords = list(STOP_WORDS)"],"metadata":{"id":"Dyp_phDqoeKS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyCustomTokenizer():\n","  def __init__(self):\n","    pass\n","  def text_data_cleaning(self, sentence):\n","    doc = nlp(sentence)\n","\n","    tokens = []\n","    for token in doc:\n","      if token.lemma_ != \"-PRON-\":\n","        temp = token.lemma_.lower().strip()\n","      else:\n","        temp = token.lower_\n","      tokens.append(temp)\n","\n","    cleaned_tokens = []\n","    for token in tokens:\n","      if token not in stopwords and token not in punct:\n","        cleaned_tokens.append(token)\n","    return cleaned_tokens"],"metadata":{"id":"hXiUpQEzotqB"},"execution_count":null,"outputs":[]}]}