{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqyYCDbKjIDJETQ5WSQBdA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hf2iiQ5l98CM","executionInfo":{"status":"ok","timestamp":1683581110776,"user_tz":-60,"elapsed":61024,"user":{"displayName":"Lawal Olanrewaju","userId":"17166837041899452081"}},"outputId":"1879ccd6-b48d-4030-cf43-665472ea885f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","source":["%cd /content/drive/My Drive/research/traffic\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ed_vQdL6-yNe","executionInfo":{"status":"ok","timestamp":1683581135108,"user_tz":-60,"elapsed":1000,"user":{"displayName":"Lawal Olanrewaju","userId":"17166837041899452081"}},"outputId":"ae8879b8-e2dd-48ef-cc41-df8f7a9a9367"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/research/traffic\n"," address.txt\t\t\t        my_spacy_annotation.ipynb\n"," annotations.json\t\t        my_tokenizer_module.ipynb\n"," best_sentiment_analysis_pipeline.pkl   my_traffic_class_module.ipynb\n"," config.cfg\t\t\t        my_word_cloud_module.ipynb\n"," data.csv\t\t\t       'NLP for Traffic Mgt.ipynb'\n"," model-best\t\t\t        __pycache__\n"," model-last\t\t\t        selected_data.txt\n"," my_clean_text_module.ipynb\t        show_traffic_levels.ipynb\n"," my_coordinates_module.ipynb\t        testing.ipynb\n"," mydata.txt\t\t\t        training.spacy\n"," my_sentiment_module.ipynb\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load data from a CSV file\n","df = pd.read_csv('data.csv', usecols=[\"date\", \"content\"]).dropna()\n","\n","# Preprocess the data by removing URLs, mentions, and hashtags\n","# df['text'] = df['text'].str.replace('http\\S+|www.\\S+|@\\S+|#\\S+', '', case=False)\n","df.info()\n"],"metadata":{"id":"JM6C2DI0BFQ6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683581135519,"user_tz":-60,"elapsed":415,"user":{"displayName":"Lawal Olanrewaju","userId":"17166837041899452081"}},"outputId":"5c3494c5-52a1-4c77-8ba2-e3ab6092ad0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 181 entries, 0 to 180\n","Data columns (total 2 columns):\n"," #   Column   Non-Null Count  Dtype \n","---  ------   --------------  ----- \n"," 0   date     181 non-null    object\n"," 1   content  181 non-null    object\n","dtypes: object(2)\n","memory usage: 3.0+ KB\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"dhrZA32yJDgP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.explode(\"content\")"],"metadata":{"id":"tE45bj_dIbby"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8o9FvQ0Iwz2","executionInfo":{"status":"ok","timestamp":1683581135974,"user_tz":-60,"elapsed":464,"user":{"displayName":"Lawal Olanrewaju","userId":"17166837041899452081"}},"outputId":"75af3868-b73b-4017-dcff-8f332590645e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                        date  \\\n","0  2023-02-09 07:12:43+00:00   \n","1  2023-02-09 07:18:48+00:00   \n","2  2023-02-09 07:18:49+00:00   \n","3  2023-02-09 07:29:07+00:00   \n","4  2023-02-09 07:29:08+00:00   \n","\n","                                             content  \n","0  Acme junction inward Agidingbi, Cadbury juncti...  \n","1  Approaching Lekki conservation centre is busy ...  \n","2  Connecting Marwa from Ikate is good Marwa back...  \n","3  Cele inward Cele/ijesha fly over bridge is bus...  \n","4  Movement from Cele inward Ago roundabout is bu...  "],"text/html":["\n","  <div id=\"df-08112f75-252d-47ba-b3a1-39d39c2b1226\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2023-02-09 07:12:43+00:00</td>\n","      <td>Acme junction inward Agidingbi, Cadbury juncti...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2023-02-09 07:18:48+00:00</td>\n","      <td>Approaching Lekki conservation centre is busy ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2023-02-09 07:18:49+00:00</td>\n","      <td>Connecting Marwa from Ikate is good Marwa back...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2023-02-09 07:29:07+00:00</td>\n","      <td>Cele inward Cele/ijesha fly over bridge is bus...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2023-02-09 07:29:08+00:00</td>\n","      <td>Movement from Cele inward Ago roundabout is bu...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08112f75-252d-47ba-b3a1-39d39c2b1226')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-08112f75-252d-47ba-b3a1-39d39c2b1226 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-08112f75-252d-47ba-b3a1-39d39c2b1226');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["df.content[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"99lHpQDnIw24","executionInfo":{"status":"ok","timestamp":1683581135975,"user_tz":-60,"elapsed":50,"user":{"displayName":"Lawal Olanrewaju","userId":"17166837041899452081"}},"outputId":"e40f5c57-ea0f-43d1-eacc-bf25f03901a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Acme junction inward Agidingbi, Cadbury junction, Mobil junction, daily times down to Coca-cola junction is good to go.     '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"vw8stPQfIw54"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Load the pre-trained GPT-2 model\n","model = pipeline('text-generation', model='gpt2')\n","\n","# Train the model on the preprocessed traffic tweets\n","with open('preprocessed_traffic_tweets.csv', 'r') as f:\n","    for line in f:\n","        model(line)"],"metadata":{"id":"dd_W2JMBBFUi","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1683581135976,"user_tz":-60,"elapsed":46,"user":{"displayName":"Lawal Olanrewaju","userId":"17166837041899452081"}},"outputId":"2f9acad7-5a37-407d-872c-4db0228b94ce"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-4d050fb48a60>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the pre-trained GPT-2 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text-generation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n","\n","# Load the pre-trained model and tokenizer\n","model_name = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n","\n","# Convert the preprocessed tweets to input features\n","tweets = pd.read_csv('preprocessed_traffic_tweets.csv')\n","inputs = tokenizer(list(tweets['text']), padding=True, truncation=True, max_length=512, return_tensors='tf')\n","\n","# Define the training data\n","dataset = tf.data.Dataset.from_tensor_slices((inputs['input_ids'], inputs['attention_mask'], tweets['label'].values))\n","\n","# Define the training parameters\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n","\n","# Train the model\n","model.fit(dataset.shuffle(len(tweets)).batch(8), epochs=3)\n"],"metadata":{"id":"HHxbDXVyBFXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Load the trained model\n","model_name = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n","\n","# Prepare the input text\n","input_text = \"There's a serious accident on the highway.\"\n","\n","# Convert the input text to input features\n","input_ids = tokenizer.encode(input_text, padding=True, truncation=True, max_length=512, return_tensors='tf')\n","input_mask = tf.ones_like(input_ids)\n","inputs = {'input_ids': input_ids, 'attention_mask': input_mask}\n","\n","# Make a prediction\n","logits = model(inputs)[0]\n","prediction = np.argmax(logits, axis=1)[0]\n","\n","# Map the prediction to the corresponding class label\n","class_labels = ['accidents', 'free-flow', 'mild traffic', 'breakdown']\n","predicted_class = class_labels[prediction]\n"],"metadata":{"id":"6cpxmzh-BFaN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3SXYAC_lKpSz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G3LCha85KpWD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.model_selection import train_test_split\n","\n","# Download necessary NLTK resources\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Define the 5 different traffic levels\n","traffic_levels = {\n","    'accident': ['accident', 'crash', 'collision', 'pile-up', \"accident\", \"casualty\", \"disaster\", \"mishap\", \"catastrophe\", \"tragedy\", \"mischance\", \"misfortune\", \"collision\", \"skidded off\"],\n","    'breakdown': ['breakdown', 'malfunction', 'engine failure', \"break\", \"broken\", \"break\", \"break down\", \"evacuation\"],\n","    'free-flow': ['free-flow', 'smooth traffic', 'no delays', \"good\", \"okay\", \"gd\", \"free\", \"encouraging\", \"gtg\"],\n","    'mild traffic': ['mild traffic', 'slight delays', 'slow-moving traffic', \"fair\",\"not so bad\",\"bit slow\", \"better\"],\n","    'heavy traffic': ['heavy traffic', 'congestion', 'gridlock', \"busy\", \"bad road\", \"bad narrow\", \"bad path\", \"traffic jam\", \"bottle-necked\", \"high side\", \"Impediment\", \"negative\", \"slow\", \"bad path\"]\n","}\n","\n","\n","# def classify_tweet(tweet_content):\n","\n","#   # Tokenize the tweet text\n","#   tokens = word_tokenize(tweet_content.lower())\n","\n","#   # Remove stop words from the tokens\n","#   stop_words = set(stopwords.words('english'))\n","#   tokens = [token for token in tokens if not token in stop_words]\n","\n","#   # Return the tokens as a list of strings\n","#   return tokens\n","\n","# # Create a new dictionary with string values\n","# traffic_levels_str = {}\n","# for key, value in traffic_levels.items():\n","#     traffic_levels_str[key] = ' '.join(value)\n","\n","# # Use the new dictionary in TfidfVectorizer\n","# vectorizer = TfidfVectorizer(vocabulary=traffic_levels_str, tokenizer=classify_tweet)\n","\n","# vectorizer.fit_transform(df['content'])\n","# print(vectorizer.vocabulary_)\n","\n","\n","\n","\n","\n","# Define a function to classify a tweet based on its content\n","def classify_tweet(tweet_content):\n","  \n","  # Tokenize the tweet text\n","  tokens = word_tokenize(tweet_content.lower())\n","\n","  # Remove stop words from the tokens\n","  stop_words = set(stopwords.words('english'))\n","  tokens = [token for token in tokens if not token in stop_words]\n","\n","  # Map the tokens to the traffic levels\n","  level = 'unknown'\n","  for key, value in traffic_levels.items():\n","    if any(token in value for token in tokens):\n","      level = key\n","      break\n","\n","  # Return the level as a string\n","  return level\n","\n","# Apply the classify_tweet function to the 'content' column of the dataframe\n","df['level'] = df['content'].apply(classify_tweet)\n","\n","# Create a new dictionary with string values\n","traffic_levels_str = {}\n","for key, value in traffic_levels.items():\n","    traffic_levels_str[key] = ' '.join(value)\n","\n","# Use the new dictionary in TfidfVectorizer\n","vectorizer = TfidfVectorizer(vocabulary=traffic_levels_str, tokenizer=classify_tweet)\n","\n","# Transform the 'content' column of the dataframe using the TfidfVectorizer\n","features = vectorizer.fit_transform(df['content'])\n","\n","# Define the labels for the machine learning algorithm\n","labels = df['level']"],"metadata":{"id":"E2rpImXZKpZg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gmaps\n","import pandas as pd\n","from sklearn.cluster import DBSCAN\n","\n","# Define the center of the map\n","center = (data['lat'].mean(), data['lng'].mean())\n","\n","# Create a numpy array of latitudes and longitudes\n","locations = data[['lat', 'lng']].values\n","\n","# Use DBSCAN to cluster the points\n","dbscan = DBSCAN(eps=0.01, min_samples=10)\n","clusters = dbscan.fit_predict(locations)\n","\n","# Calculate the density of each cluster\n","cluster_counts = dict()\n","for cluster_id in set(clusters):\n","    if cluster_id == -1:\n","        continue\n","    cluster_locations = locations[clusters == cluster_id]\n","    density = len(cluster_locations) / (gmaps.distance_matrix(cluster_locations, cluster_locations).min(axis=1).mean()**2)\n","    cluster_counts[cluster_id] = density\n","\n","# Create the map figure\n","fig = gmaps.figure(center=center, zoom_level=12)\n","\n","# Create a list of colors for the clusters\n","# colors = ['red', 'green', 'blue', 'yellow', 'purple', 'orange', 'pink', 'brown', 'gray', 'olive']\n","colors = ['rgba(255, 0, 0, 0)', 'rgba(255, 0, 0, 1)', 'rgba(255, 191, 0, 1)', 'rgba(255, 255, 0, 1)', 'rgba(0, 255, 0, 1)']\n","legend_entries = [(colors[i], str(i)) for i in range(len(colors))]\n","\n","\n","# Create a list of markers for each cluster\n","markers = []\n","for cluster_id, color in zip(set(clusters), colors):\n","    if cluster_id == -1:\n","        color = 'black'\n","        continue  # Skip noise cluster\n","    if cluster_id not in cluster_counts:  # Skip cluster if it has no density value\n","        continue\n","    cluster_locations = locations[clusters == cluster_id]\n","    if len(cluster_locations) == 0:\n","        continue\n","    marker_layer = gmaps.symbol_layer(\n","        cluster_locations,\n","        fill_color=color,\n","        stroke_color=color,\n","        scale=2,\n","        info_box_content=str(cluster_counts[cluster_id]),\n","    )\n","    markers.append(marker_layer)\n","\n","\n","# Add the marker layers to the map\n","fig.add_layer(gmaps.heatmap_layer(locations))\n","for marker_layer in markers:\n","    fig.add_layer(marker_layer)\n","\n","# Create the legend\n","legend_markers = []\n","for color, label in legend_entries:\n","    legend_markers.append(gmaps.Layer(\n","        gmaps.Symbol(\n","            location=(0, 0),\n","            stroke_color=color,\n","            fill_color=color,\n","            scale=2,\n","            label=label,\n","            anchor=(0, -2)\n","        )\n","    ))\n","    \n","legend = gmaps.Map(layout={'width': '400px', 'height': '300px'})\n","legend.layers = legend_markers\n","\n","fig.add_layer(gmaps.symbol_layer(\n","    locations, fill_color=colors, stroke_color=colors, scale=2, \n","    info_box_content=locations_info, display_info_box=True))\n","fig.add_layer(legend)\n","\n","\n","# Display the map\n","fig\n"],"metadata":{"id":"yxJm5Rk6Kph4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QGeeJuQPKpkn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QM6JjORsKpoA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5BoX5jykBpO9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8PuJGkWABpSO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0gSfrhbPBpVQ"},"execution_count":null,"outputs":[]}]}